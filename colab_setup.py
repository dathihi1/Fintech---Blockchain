"""
Smart Trading Journal - Colab Training Script
DÃ¹ng Kaggle API + LÆ°u config vÃ o Google Drive

HÆ°á»›ng dáº«n:
1. VÃ o https://colab.new
2. Copy toÃ n bá»™ code nÃ y vÃ o cell
3. Runtime > Change runtime type > T4 GPU
4. Nháº¥n Shift+Enter Ä‘á»ƒ cháº¡y
"""

# ============================================
# BÆ¯á»šC 1: CÃ i Ä‘áº·t vÃ  kiá»ƒm tra GPU
# ============================================
print("=" * 50)
print("ğŸš€ Smart Trading Journal - Model Training")
print("=" * 50)

import subprocess
subprocess.run(['pip', 'install', '-q', 'torch', 'transformers', 'datasets', 
                'scikit-learn', 'pandas', 'numpy', 'kaggle', 'accelerate'])

import torch
print(f"\nâœ… PyTorch: {torch.__version__}")
print(f"âœ… CUDA: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"âœ… GPU: {torch.cuda.get_device_name(0)}")

# ============================================
# BÆ¯á»šC 2: Mount Google Drive & Setup Kaggle
# ============================================
print("\n" + "=" * 50)
print("ï¿½ Mount Google Drive")
print("=" * 50)

from google.colab import drive
import os
import json

# Mount Drive
drive.mount('/content/drive')

# ÄÆ°á»ng dáº«n lÆ°u config trÃªn Drive
KAGGLE_CONFIG_DIR = '/content/drive/MyDrive/colab_configs'
KAGGLE_JSON_PATH = f'{KAGGLE_CONFIG_DIR}/kaggle.json'

os.makedirs(KAGGLE_CONFIG_DIR, exist_ok=True)
os.makedirs('/root/.kaggle', exist_ok=True)

# Kiá»ƒm tra xem Ä‘Ã£ cÃ³ config chÆ°a
if os.path.exists(KAGGLE_JSON_PATH):
    print("âœ… TÃ¬m tháº¥y Kaggle config trÃªn Drive!")
    os.system(f'cp {KAGGLE_JSON_PATH} /root/.kaggle/kaggle.json')
    os.chmod('/root/.kaggle/kaggle.json', 0o600)
    print("âœ… ÄÃ£ load Kaggle config tá»« Drive")
else:
    print("âš ï¸ ChÆ°a cÃ³ Kaggle config. Äang táº¡o má»›i...")
    
    # Táº¡o config vá»›i API token má»›i (dáº¡ng KGAT_xxx)
    # Kaggle hiá»‡n dÃ¹ng format username + API token
    print("""
ğŸ“ CÃ¡ch láº¥y thÃ´ng tin:
1. VÃ o https://www.kaggle.com/settings
2. Scroll xuá»‘ng má»¥c "API" 
3. Báº¡n cáº§n:
   - Username: tÃªn tÃ i khoáº£n Kaggle cá»§a báº¡n
   - API Token: dáº¡ng KGAT_xxxxxxx (nhÆ° báº¡n Ä‘Ã£ cÃ³)
    """)
    
    kaggle_username = input("Nháº­p Kaggle username: ")
    kaggle_token = input("Nháº­p API Token (KGAT_xxx): ")
    
    # Táº¡o kaggle.json
    kaggle_config = {
        "username": kaggle_username,
        "key": kaggle_token
    }
    
    # LÆ°u vÃ o Drive (Ä‘á»ƒ dÃ¹ng láº§n sau)
    with open(KAGGLE_JSON_PATH, 'w') as f:
        json.dump(kaggle_config, f)
    print(f"âœ… ÄÃ£ lÆ°u config vÃ o Drive: {KAGGLE_JSON_PATH}")
    
    # Copy vÃ o thÆ° má»¥c kaggle
    os.system(f'cp {KAGGLE_JSON_PATH} /root/.kaggle/kaggle.json')
    os.chmod('/root/.kaggle/kaggle.json', 0o600)
    print("âœ… Config Ä‘Ã£ Ä‘Æ°á»£c setup!")

print("\nğŸ’¡ Láº§n sau config sáº½ tá»± Ä‘á»™ng load tá»« Drive!")

# ============================================
# BÆ¯á»šC 3: Download Dataset tá»« Kaggle
# ============================================
print("\n" + "=" * 50)
print("ï¿½ Download Dataset tá»« Kaggle")
print("=" * 50)

# Set environment variable cho API Token má»›i
os.environ['KAGGLE_API_TOKEN'] = open('/root/.kaggle/kaggle.json').read()

# Download Financial Sentiment dataset
print("\nğŸ“¦ Äang táº£i dataset...")
os.system('kaggle datasets download -d ankurzing/sentiment-analysis-for-financial-news -p /content/data --unzip')

# Load data
import pandas as pd

data_path = '/content/data/all-data.csv'
if os.path.exists(data_path):
    df = pd.read_csv(data_path, encoding='latin-1', header=None, names=['sentiment', 'text'])
    print(f"âœ… Loaded {len(df)} samples")
else:
    # Thá»­ tÃ¬m file khÃ¡c
    print("âš ï¸ Äang tÃ¬m file data...")
    os.system('ls -la /content/data/')
    
    # Fallback: dÃ¹ng HuggingFace
    print("\nğŸ“¦ Fallback: Táº£i tá»« HuggingFace...")
    from datasets import load_dataset
    dataset = load_dataset("zeroshot/twitter-financial-news-sentiment")
    df = pd.DataFrame({
        'text': dataset['train']['text'],
        'sentiment': [['negative', 'positive', 'neutral'][l] for l in dataset['train']['label']]
    })
    print(f"âœ… Loaded {len(df)} samples tá»« HuggingFace")

# Map labels
sentiment_map = {'negative': 0, 'neutral': 1, 'positive': 2}
df['label'] = df['sentiment'].map(sentiment_map)
df = df.dropna()

print(f"\nğŸ“Š Label distribution:")
print(df['sentiment'].value_counts())

# ============================================
# BÆ¯á»šC 4: Chuáº©n bá»‹ & Training
# ============================================
print("\n" + "=" * 50)
print("ï¿½ Preparing Data & Training")
print("=" * 50)

import numpy as np
from sklearn.model_selection import train_test_split
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)
from datasets import Dataset
from sklearn.metrics import accuracy_score, classification_report

# Config
CONFIG = {
    "base_model": "ProsusAI/finbert",
    "learning_rate": 2e-5,
    "batch_size": 16,
    "epochs": 3,
    "max_length": 128,
    "sentiment_labels": ["negative", "neutral", "positive"]
}

# Split
train_texts, test_texts, train_labels, test_labels = train_test_split(
    df['text'].tolist(), df['label'].tolist(), 
    test_size=0.2, random_state=42
)
print(f"âœ… Train: {len(train_texts)}, Test: {len(test_texts)}")

# Load model
print(f"\nğŸ“¦ Loading {CONFIG['base_model']}...")
tokenizer = AutoTokenizer.from_pretrained(CONFIG['base_model'])
model = AutoModelForSequenceClassification.from_pretrained(
    CONFIG['base_model'], num_labels=3
)

# Tokenize
def tokenize_fn(examples):
    return tokenizer(examples['text'], padding='max_length', 
                     truncation=True, max_length=128)

train_ds = Dataset.from_dict({"text": train_texts, "label": train_labels})
test_ds = Dataset.from_dict({"text": test_texts, "label": test_labels})
train_ds = train_ds.map(tokenize_fn, batched=True)
test_ds = test_ds.map(tokenize_fn, batched=True)

# Training
print("\nğŸš€ TRAINING (5-10 phÃºt)...")

def compute_metrics(p):
    preds = np.argmax(p.predictions, axis=1)
    return {'accuracy': accuracy_score(p.label_ids, preds)}

trainer = Trainer(
    model=model,
    args=TrainingArguments(
        output_dir="./results",
        learning_rate=2e-5,
        per_device_train_batch_size=16,
        num_train_epochs=3,
        eval_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        fp16=torch.cuda.is_available(),
        report_to="none"
    ),
    train_dataset=train_ds,
    eval_dataset=test_ds,
    tokenizer=tokenizer,
    data_collator=DataCollatorWithPadding(tokenizer),
    compute_metrics=compute_metrics
)

trainer.train()

# Evaluate
print("\nğŸ“Š Results:")
preds = np.argmax(trainer.predict(test_ds).predictions, axis=1)
print(classification_report(test_labels, preds, target_names=CONFIG['sentiment_labels']))

# ============================================
# BÆ¯á»šC 5: Save Model (cáº£ local vÃ  Drive)
# ============================================
print("\n" + "=" * 50)
print("ğŸ’¾ Saving Model")
print("=" * 50)

# Save local
output_dir = "./finbert_trading_vi"
trainer.save_model(output_dir)
tokenizer.save_pretrained(output_dir)

# Save to Drive
drive_model_path = '/content/drive/MyDrive/colab_configs/finbert_trading_vi'
os.system(f'cp -r {output_dir} {drive_model_path}')
print(f"âœ… Model saved to Drive: {drive_model_path}")

# Zip & Download
os.system(f"zip -r finbert_trading_vi.zip {output_dir}/")

from google.colab import files
print("\nğŸ“¥ Downloading...")
files.download('finbert_trading_vi.zip')

print("\n" + "=" * 50)
print("âœ… HOÃ€N Táº¤T!")
print("=" * 50)
print("""
ğŸ“ Model Ä‘Ã£ Ä‘Æ°á»£c lÆ°u:
   - Google Drive: /MyDrive/colab_configs/finbert_trading_vi/
   - Downloaded: finbert_trading_vi.zip

ğŸ“Œ Copy vÃ o: backend/ml/models/finbert_trading_vi/
""")
